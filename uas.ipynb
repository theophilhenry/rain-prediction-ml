{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --\n",
    "# Read Data\n",
    "# --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/weatherAUS.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --\n",
    "# Data Preprocessing\n",
    "# --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.concatenate((X, np.reshape(y, (-1, 1))), axis=1)\n",
    "t = (data.dtypes == \"float64\")\n",
    "num_cols = list(t[t].index)\n",
    "\n",
    "# Ada Berapa Data Null di Column Numeric?\n",
    "totalNull = 0\n",
    "for i in num_cols:\n",
    "    # print(i, data[i].isnull().sum())\n",
    "    break\n",
    "    \n",
    "# Fill NA\n",
    "for i in num_cols:\n",
    "    data[i].fillna(data[i].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah agar format tanggal bisa diproses dengan memisahkan tahun, bulan, dan tanggalnya\n",
    "data['Date']= pd.to_datetime(data[\"Date\"])\n",
    "data['year'] = data.Date.dt.year\n",
    "data['month'] = data.Date.dt.month\n",
    "data['day'] = data.Date.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Mendapatkan kolom yang bertipe categorical\n",
    "categorical_columns = (data.dtypes == \"object\")\n",
    "object_columns = list(categorical_columns[categorical_columns].index)\n",
    "\n",
    "# Melengkapi data dari kolom categorical yang memiliki nilai null\n",
    "for i in object_columns:\n",
    "    data[i].fillna(data[i].mode()[0], inplace=True)\n",
    "\n",
    "# Label Encoder untuk mengubah kolom categorical menjadi numerik\n",
    "label_encoder = LabelEncoder()\n",
    "for i in object_columns:\n",
    "    data[i] = label_encoder.fit_transform(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengubah format data frame menjadi numpy\n",
    "dataNP = data.to_numpy()\n",
    "\n",
    "# Mengambil index kolom yang akan digunakan sebagai data X\n",
    "idx = []\n",
    "for i in range(26):\n",
    "    # Index ke 0 yaitu date tidak digunakan karena sudah dipecah menjadi kolom day, month, year. Index 22 tidak digunakan karena merupakan target\n",
    "    if i == 0 or i == 22:\n",
    "        continue\n",
    "    idx.append(i)         \n",
    "\n",
    "# Inisialisasi X dan y\n",
    "X = dataNP[:, idx]\n",
    "y = dataNP[:, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87276, 24)\n",
      "(58184, 24)\n",
      "(87276,)\n",
      "(58184,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split untuk data training dan testing\n",
    "xtrain, xtest, ytrain, ytest = (train_test_split(X, y,test_size = 0.4,random_state=1))\n",
    "\n",
    "print(xtrain.shape)\n",
    "print(xtest.shape)\n",
    "print(ytrain.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.99)\n",
    "xtrain = pca.fit_transform(xtrain)\n",
    "xtest = pca.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IQR Detecting Outlier\n",
    "# data = np.concatenate((xtrain, np.reshape(ytrain, (-1, 1))), axis=1)\n",
    "\n",
    "# num_rows, num_cols = xtrain.shape\n",
    "\n",
    "# for out in range(num_cols):\n",
    "#   colData = data[:,out]\n",
    "#   q1,q3 = np.percentile(colData,[25,75])\n",
    "#   iqr = q3-q1\n",
    "#   lb,ub = q1-1.5*iqr, q3+1.5*iqr\n",
    "\n",
    "#   data = data[np.where((colData >= lb) & (colData <= ub))]\n",
    "\n",
    "# xtrain = data[:,0:num_cols]\n",
    "# ytrain = data[:,num_cols]\n",
    "\n",
    "# print(xtrain.shape)\n",
    "# print(ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gabungkan Xtrain dan ytrain untuk detecting outlier\n",
    "# data = np.concatenate((xtrain, np.reshape(ytrain, (-1, 1))), axis=1)\n",
    "\n",
    "# num_rows, num_cols = xtrain.shape\n",
    "\n",
    "# for idx in range(0, num_cols):\n",
    "#   col_data = data[:, idx]\n",
    "  \n",
    "#   mean = col_data.mean()\n",
    "#   std = col_data.std()\n",
    "#   lb, ub = mean-(3*std), mean+(3*std)\n",
    "\n",
    "#   data = data[np.where((col_data >= lb) & (col_data <= ub))]\n",
    "\n",
    "# # Pisahkan Xtrain dan ytrain yang sebelumnya digabung\n",
    "# xtrain = data[:, 0:num_cols]\n",
    "# ytrain = data[:, num_cols]\n",
    "\n",
    "# print(xtrain.shape)\n",
    "# print(ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Proses Standardization\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(xtrain)\n",
    "# xtrain = scaler.transform(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Proses MinMax Scaler\n",
    "# scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "# scaler.fit(xtrain)\n",
    "# xtrain = scaler.transform(xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron in hidden layer: 10, Accuracy: 84.92 %\n",
      "Confusion Matrix: \n",
      "[[43142  2448]\n",
      " [ 6326  6268]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "xtrain = xtrain.astype('float')\n",
    "ytrain = ytrain.astype('float')\n",
    "xtest = xtest.astype('float')\n",
    "ytest = ytest.astype('float')\n",
    "\n",
    "for i in range(2, 11):\n",
    "    ann = MLPClassifier(hidden_layer_sizes=(i,), max_iter=10000, random_state=1)\n",
    "    ann.fit(xtrain, ytrain)\n",
    "    ypred = ann.predict(xtest)\n",
    "    acc_score = 100 * accuracy_score(ytest, ypred)\n",
    "    cm = confusion_matrix(ytest, ypred)\n",
    "    print(\"Neuron in hidden layer: %g, Accuracy: %.2f %%\"%(i,acc_score))\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(cm)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
